MPI startup(): Warning: I_MPI_PMI_LIBRARY will be ignored since the hydra process manager was found
MPI startup(): Warning: I_MPI_PMI_LIBRARY will be ignored since the hydra process manager was found
MPI startup(): Warning: I_MPI_PMI_LIBRARY will be ignored since the hydra process manager was found
MPI startup(): Warning: I_MPI_PMI_LIBRARY will be ignored since the hydra process manager was found
MPI startup(): Warning: I_MPI_PMI_LIBRARY will be ignored since the hydra process manager was found
MPI startup(): Warning: I_MPI_PMI_LIBRARY will be ignored since the hydra process manager was found
MPI startup(): Warning: I_MPI_PMI_LIBRARY will be ignored since the hydra process manager was found
MPI startup(): Warning: I_MPI_PMI_LIBRARY will be ignored since the hydra process manager was found
MPI startup(): Warning: I_MPI_PMI_LIBRARY will be ignored since the hydra process manager was found
MPI startup(): Warning: I_MPI_PMI_LIBRARY will be ignored since the hydra process manager was found
MPI startup(): Warning: I_MPI_PMI_LIBRARY will be ignored since the hydra process manager was found
MPI startup(): Warning: I_MPI_PMI_LIBRARY will be ignored since the hydra process manager was found
MPI startup(): Warning: I_MPI_PMI_LIBRARY will be ignored since the hydra process manager was found
MPI startup(): Warning: I_MPI_PMI_LIBRARY will be ignored since the hydra process manager was found
MPI startup(): Warning: I_MPI_PMI_LIBRARY will be ignored since the hydra process manager was found
MPI startup(): Warning: I_MPI_PMI_LIBRARY will be ignored since the hydra process manager was found
MPI startup(): Warning: I_MPI_PMI_LIBRARY will be ignored since the hydra process manager was found
MPI startup(): Warning: I_MPI_PMI_LIBRARY will be ignored since the hydra process manager was found
MPI startup(): Warning: I_MPI_PMI_LIBRARY will be ignored since the hydra process manager was found
MPI startup(): Warning: I_MPI_PMI_LIBRARY will be ignored since the hydra process manager was found
MPI startup(): Warning: I_MPI_PMI_LIBRARY will be ignored since the hydra process manager was found
MPI startup(): Warning: I_MPI_PMI_LIBRARY will be ignored since the hydra process manager was found
MPI startup(): Warning: I_MPI_PMI_LIBRARY will be ignored since the hydra process manager was found
MPI startup(): Warning: I_MPI_PMI_LIBRARY will be ignored since the hydra process manager was found
MPI startup(): Warning: I_MPI_PMI_LIBRARY will be ignored since the hydra process manager was found
MPI startup(): Warning: I_MPI_PMI_LIBRARY will be ignored since the hydra process manager was found
MPI startup(): Warning: I_MPI_PMI_LIBRARY will be ignored since the hydra process manager was found
MPI startup(): Warning: I_MPI_PMI_LIBRARY will be ignored since the hydra process manager was found
MPI startup(): Warning: I_MPI_PMI_LIBRARY will be ignored since the hydra process manager was found
MPI startup(): Warning: I_MPI_PMI_LIBRARY will be ignored since the hydra process manager was found
MPI startup(): Warning: I_MPI_PMI_LIBRARY will be ignored since the hydra process manager was found
MPI startup(): Warning: I_MPI_PMI_LIBRARY will be ignored since the hydra process manager was found
MPI startup(): Warning: I_MPI_PMI_LIBRARY will be ignored since the hydra process manager was found
MPI startup(): Warning: I_MPI_PMI_LIBRARY will be ignored since the hydra process manager was found
MPI startup(): Warning: I_MPI_PMI_LIBRARY will be ignored since the hydra process manager was found
MPI startup(): Warning: I_MPI_PMI_LIBRARY will be ignored since the hydra process manager was found

     Program PWSCF v.6.4.1 starts on  6Dec2023 at 18: 6:34 

     This program is part of the open-source Quantum ESPRESSO suite
     for quantum simulation of materials; please cite
         "P. Giannozzi et al., J. Phys.:Condens. Matter 21 395502 (2009);
         "P. Giannozzi et al., J. Phys.:Condens. Matter 29 465901 (2017);
          URL http://www.quantum-espresso.org", 
     in publications or presentations arising from this work. More details at
     http://www.quantum-espresso.org/quote

     Parallel version (MPI), running on    36 processors

     MPI processes distributed on     2 nodes
     R & G space division:  proc/nbgrp/npool/nimage =      36
     Waiting for input...
     Reading input from standard input
Warning: card &IONS ignored
Warning: card / ignored

     Current dimensions of program PWSCF are:
     Max number of different atomic species (ntypx) = 10
     Max number of k-points (npk) =  40000
     Max angular momentum in pseudopotentials (lmaxx) =  3

     Atomic positions and unit cell read from directory:
     ./temp/ZnO.save/
 

     IMPORTANT: XC functional enforced from input :
     Exchange-correlation      = WC ( 1  4 11  4 0 0)
     Any further DFT definition will be discarded
     Please, verify this is what you really want


     Subspace diagonalization in iterative solution of the eigenvalue problem:
     a serial algorithm will be used

 
     Parallelization info
     --------------------
     sticks:   dense  smooth     PW     G-vecs:    dense   smooth      PW
     Min          22      11      3                  852      301      53
     Max          23      12      4                  863      308      62
     Sum         823     421    139                30903    10995    2081
 


     bravais-lattice index     =            4
     lattice parameter (alat)  =       6.1180  a.u.
     unit-cell volume          =     320.1818 (a.u.)^3
     number of atoms/cell      =            4
     number of atomic types    =            2
     number of electrons       =        52.00
     number of Kohn-Sham states=           30
     kinetic-energy cutoff     =      40.0000  Ry
     charge density cutoff     =     320.0000  Ry
     Exchange-correlation      = WC ( 1  4 11  4 0 0)

     celldm(1)=   6.117960  celldm(2)=   0.000000  celldm(3)=   1.614530
     celldm(4)=   0.000000  celldm(5)=   0.000000  celldm(6)=   0.000000

     crystal axes: (cart. coord. in units of alat)
               a(1) = (   1.000000   0.000000   0.000000 )  
               a(2) = (  -0.500000   0.866025   0.000000 )  
               a(3) = (   0.000000   0.000000   1.614530 )  

     reciprocal axes: (cart. coord. in units 2 pi/alat)
               b(1) = (  1.000000  0.577350  0.000000 )  
               b(2) = (  0.000000  1.154701  0.000000 )  
               b(3) = (  0.000000  0.000000  0.619375 )  


     PseudoPot. # 1 for Zn read from file:
     /umbc/xfs1/bennettj/common/GBRV-PSP/zn_pbe_v1.uspp.F.UPF
     MD5 check sum: df62231357ef9e81f77b2b3087fa5675
     Pseudo is Ultrasoft, Zval = 20.0
     Generated by new atomic code, or converted to UPF format
     Using radial grid of  943 points,  6 beta functions with: 
                l(1) =   0
                l(2) =   0
                l(3) =   1
                l(4) =   1
                l(5) =   2
                l(6) =   2
     Q(r) pseudized with  8 coefficients,  rinner =    1.100   1.100   1.100
                                                       1.100   1.100

     PseudoPot. # 2 for O  read from file:
     /umbc/xfs1/bennettj/common/GBRV-PSP/o_pbe_v1.2.uspp.F.UPF
     MD5 check sum: 734c27235a0248c51dbae37a1fbe46ec
     Pseudo is Ultrasoft + core correction, Zval =  6.0
     Generated by new atomic code, or converted to UPF format
     Using radial grid of  737 points,  5 beta functions with: 
                l(1) =   0
                l(2) =   0
                l(3) =   1
                l(4) =   1
                l(5) =   2
     Q(r) pseudized with  8 coefficients,  rinner =    0.900   0.900   0.900
                                                       0.900   0.900

     atomic species   valence    mass     pseudopotential
        Zn            20.00    65.38000     Zn( 1.00)
        O              6.00    15.99900     O ( 1.00)

     12 Sym. Ops. (no inversion) found ( 6 have fractional translation)



   Cartesian axes

     site n.     atom                  positions (alat units)
         1           Zn  tau(   1) = (  -0.0000014   0.5773511   0.0029280  )
         2           Zn  tau(   2) = (   0.5000014   0.2886743   0.8101930  )
         3           O   tau(   3) = (  -0.0000001   0.5773503   0.6146298  )
         4           O   tau(   4) = (   0.5000001   0.2886751   1.4218948  )

     number of k points=   150  gaussian smearing, width (Ry)=  0.0100

     Number of k-points >= 100: set verbosity='high' to print them.

     Dense  grid:    30903 G-vectors     FFT dimensions: (  36,  36,  60)

     Smooth grid:    10995 G-vectors     FFT dimensions: (  25,  25,  40)

     Estimated max dynamical RAM per process >       5.63 MB

     Estimated total dynamical RAM >     202.60 MB

     The potential is recalculated from file :
     ./temp/ZnO.save/charge-density

     Starting wfcs are   34 randomized atomic wfcs

     Band Structure Calculation
     Davidson diagonalization with overlap

     ethr =  1.92E-10,  avg # of iterations = 16.2

     total cpu time spent up to now is       14.8 secs

     End of band structure calculation

     Number of k-points >= 100: set verbosity='high' to print the bands.

     the Fermi energy is     9.1827 ev

     Writing output data file ZnO.save/
 
     init_run     :      0.05s CPU      0.10s WALL (       1 calls)
     electrons    :     13.92s CPU     14.53s WALL (       1 calls)

     Called by init_run:
     wfcinit      :      0.00s CPU      0.02s WALL (       1 calls)
     potinit      :      0.00s CPU      0.02s WALL (       1 calls)
     hinit0       :      0.02s CPU      0.02s WALL (       1 calls)

     Called by electrons:
     c_bands      :     13.92s CPU     14.53s WALL (       1 calls)
     v_of_rho     :      0.00s CPU      0.00s WALL (       1 calls)
     newd         :      0.02s CPU      0.02s WALL (       1 calls)

     Called by c_bands:
     init_us_2    :      0.01s CPU      0.01s WALL (     150 calls)
     cegterg      :     12.58s CPU     13.09s WALL (     157 calls)

     Called by sum_band:

     Called by *egterg:
     h_psi        :      7.82s CPU      8.27s WALL (    2741 calls)
     s_psi        :      0.05s CPU      0.06s WALL (    2741 calls)
     g_psi        :      0.01s CPU      0.01s WALL (    2434 calls)
     cdiaghg      :      4.80s CPU      4.90s WALL (    2584 calls)

     Called by h_psi:
     h_psi:pot    :      7.80s CPU      8.25s WALL (    2741 calls)
     h_psi:calbec :      0.33s CPU      0.35s WALL (    2741 calls)
     vloc_psi     :      7.39s CPU      7.81s WALL (    2741 calls)
     add_vuspsi   :      0.06s CPU      0.07s WALL (    2741 calls)

     General routines
     calbec       :      0.32s CPU      0.34s WALL (    2741 calls)
     fft          :      0.02s CPU      0.02s WALL (      13 calls)
     ffts         :      0.00s CPU      0.01s WALL (       1 calls)
     fftw         :      7.15s CPU      7.54s WALL (   84000 calls)
     interpolate  :      0.00s CPU      0.01s WALL (       1 calls)
     davcio       :      0.01s CPU      0.01s WALL (     300 calls)
 
     Parallel routines
     fft_scatt_xy :      0.25s CPU      0.28s WALL (   84014 calls)
     fft_scatt_yz :      5.68s CPU      5.94s WALL (   84014 calls)
 
     PWSCF        :     14.80s CPU     16.67s WALL

 
   This run was terminated on:  18: 6:51   6Dec2023            

=------------------------------------------------------------------------------=
   JOB DONE.
=------------------------------------------------------------------------------=
